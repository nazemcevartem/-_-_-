{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import lightgbm as lgb\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from pathlib import Path\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "E4dJ7A-vj8Um"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR = Path(\".\")\n",
        "DATA_DIR = ROOT_DIR / \"data\"\n",
        "RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
        "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
        "MODEL_DIR = ROOT_DIR / \"models\"\n",
        "SUBMISSION_DIR = ROOT_DIR / \"submissions\"\n",
        "\n",
        "TRAIN_FILENAME = \"train.csv\"\n",
        "TEST_FILENAME = \"test.csv\"\n",
        "USER_DATA_FILENAME = \"users.csv\"\n",
        "BOOK_DATA_FILENAME = \"books.csv\"\n",
        "BOOK_GENRES_FILENAME = \"book_genres.csv\"\n",
        "GENRES_FILENAME = \"genres.csv\"\n",
        "BOOK_DESCRIPTIONS_FILENAME = \"book_descriptions.csv\"\n",
        "PROCESSED_DATA_FILENAME = \"processed_features.parquet\"\n",
        "MODEL_FILENAME = \"lgb_model.txt\"\n",
        "\n",
        "COL_USER_ID = \"user_id\"\n",
        "COL_BOOK_ID = \"book_id\"\n",
        "COL_TARGET = \"rating\"\n",
        "COL_SOURCE = \"source\"\n",
        "COL_PREDICTION = \"rating_predict\"\n",
        "COL_HAS_READ = \"has_read\"\n",
        "COL_TIMESTAMP = \"timestamp\"\n",
        "COL_GENDER = \"gender\"\n",
        "COL_AGE = \"age\"\n",
        "COL_AUTHOR_ID = \"author_id\"\n",
        "COL_PUBLICATION_YEAR = \"publication_year\"\n",
        "COL_LANGUAGE = \"language\"\n",
        "COL_PUBLISHER = \"publisher\"\n",
        "COL_AVG_RATING = \"avg_rating\"\n",
        "COL_GENRE_ID = \"genre_id\"\n",
        "COL_DESCRIPTION = \"description\"\n",
        "COL_RATING = \"rating\"\n",
        "\n",
        "F_USER_MEAN_RATING = \"user_mean_rating\"\n",
        "F_USER_RATINGS_COUNT = \"user_ratings_count\"\n",
        "F_BOOK_MEAN_RATING = \"book_mean_rating\"\n",
        "F_BOOK_RATINGS_COUNT = \"book_ratings_count\"\n",
        "F_AUTHOR_MEAN_RATING = \"author_mean_rating\"\n",
        "F_BOOK_GENRES_COUNT = \"book_genres_count\"\n",
        "\n",
        "VAL_SOURCE_TRAIN = \"train\"\n",
        "VAL_SOURCE_TEST = \"test\"\n",
        "MISSING_CAT_VALUE = \"-1\"\n",
        "MISSING_NUM_VALUE = -1\n",
        "PREDICTION_MIN_VALUE = 0\n",
        "PREDICTION_MAX_VALUE = 10\n",
        "\n",
        "BERT_MODEL_NAME = \"DeepPavlov/rubert-base-cased\"\n",
        "BERT_BATCH_SIZE = 8\n",
        "BERT_MAX_LENGTH = 512\n",
        "BERT_EMBEDDING_DIM = 768\n",
        "BERT_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "LGB_PARAMS = {\n",
        "    \"objective\": \"rmse\",\n",
        "    \"metric\": \"rmse\",\n",
        "    \"n_estimators\": 2000,\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"feature_fraction\": 0.8,\n",
        "    \"bagging_fraction\": 0.8,\n",
        "    \"bagging_freq\": 1,\n",
        "    \"lambda_l1\": 0.1,\n",
        "    \"lambda_l2\": 0.1,\n",
        "    \"num_leaves\": 31,\n",
        "    \"verbose\": -1,\n",
        "    \"n_jobs\": -1,\n",
        "    \"seed\": 42,\n",
        "    \"boosting_type\": \"gbdt\",\n",
        "}\n",
        "\n",
        "CAT_FEATURES = [\n",
        "    COL_USER_ID, COL_BOOK_ID, COL_GENDER, COL_AGE, COL_AUTHOR_ID,\n",
        "    COL_PUBLICATION_YEAR, COL_LANGUAGE, COL_PUBLISHER\n",
        "]"
      ],
      "metadata": {
        "id": "n538zwApkTz4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = \"/content/stage1_individual_data.zip\"\n",
        "extract_dir = Path(\"data/raw\")\n",
        "extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(\"Данные разархивированы в:\", extract_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LewOVYoBk-L6",
        "outputId": "de87d419-7dac-419e-df61-d8607d0e3ead"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Данные разархивированы в: data/raw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_merge_data():\n",
        "    dtype_spec = {\n",
        "        COL_USER_ID: \"int32\",\n",
        "        COL_BOOK_ID: \"int32\",\n",
        "        COL_TARGET: \"float32\",\n",
        "        COL_GENDER: \"category\",\n",
        "        COL_AGE: \"float32\",\n",
        "        COL_AUTHOR_ID: \"int32\",\n",
        "        COL_PUBLICATION_YEAR: \"float32\",\n",
        "        COL_LANGUAGE: \"category\",\n",
        "        COL_PUBLISHER: \"category\",\n",
        "        COL_AVG_RATING: \"float32\",\n",
        "        COL_GENRE_ID: \"int16\",\n",
        "    }\n",
        "\n",
        "    train_df = pd.read_csv(RAW_DATA_DIR / TRAIN_FILENAME,\n",
        "                           dtype={k:v for k,v in dtype_spec.items() if k in [COL_USER_ID, COL_BOOK_ID, COL_TARGET]},\n",
        "                           parse_dates=[COL_TIMESTAMP])\n",
        "    train_df = train_df[train_df[COL_HAS_READ]==1].copy()\n",
        "    test_df = pd.read_csv(RAW_DATA_DIR / TEST_FILENAME,\n",
        "                          dtype={k:v for k,v in dtype_spec.items() if k in [COL_USER_ID, COL_BOOK_ID]})\n",
        "    user_data_df = pd.read_csv(RAW_DATA_DIR / USER_DATA_FILENAME,\n",
        "                               dtype={k:v for k,v in dtype_spec.items() if k in [COL_USER_ID, COL_GENDER, COL_AGE]})\n",
        "    book_data_df = pd.read_csv(RAW_DATA_DIR / BOOK_DATA_FILENAME,\n",
        "                               dtype={k:v for k,v in dtype_spec.items() if k in [COL_BOOK_ID, COL_AUTHOR_ID, COL_PUBLICATION_YEAR, COL_LANGUAGE, COL_AVG_RATING, COL_PUBLISHER]})\n",
        "    book_genres_df = pd.read_csv(RAW_DATA_DIR / BOOK_GENRES_FILENAME,\n",
        "                                 dtype={k:v for k,v in dtype_spec.items() if k in [COL_BOOK_ID, COL_GENRE_ID]})\n",
        "    genres_df = pd.read_csv(RAW_DATA_DIR / GENRES_FILENAME)\n",
        "    book_descriptions_df = pd.read_csv(RAW_DATA_DIR / BOOK_DESCRIPTIONS_FILENAME,\n",
        "                                       dtype={COL_BOOK_ID:\"int32\"})\n",
        "\n",
        "    train_df[COL_SOURCE] = VAL_SOURCE_TRAIN\n",
        "    test_df[COL_SOURCE] = VAL_SOURCE_TEST\n",
        "    combined_df = pd.concat([train_df, test_df], ignore_index=True, sort=False)\n",
        "    combined_df = combined_df.merge(user_data_df, on=COL_USER_ID, how=\"left\")\n",
        "    book_data_df = book_data_df.drop_duplicates(subset=[COL_BOOK_ID])\n",
        "    combined_df = combined_df.merge(book_data_df, on=COL_BOOK_ID, how=\"left\")\n",
        "\n",
        "    return combined_df, book_genres_df, genres_df, book_descriptions_df\n",
        "\n",
        "merged_df, book_genres_df, genres_df, descriptions_df = load_and_merge_data()\n",
        "print(\"Данные загружены и объединены\")\n",
        "\n",
        "# === Временной сплит ===\n",
        "def get_split_date_from_ratio(df, ratio, timestamp_col=COL_TIMESTAMP):\n",
        "    sorted_ts = df[timestamp_col].sort_values()\n",
        "    threshold_index = int(len(sorted_ts)*ratio)\n",
        "    return sorted_ts.iloc[threshold_index]\n",
        "\n",
        "def temporal_split_by_date(df, split_date, timestamp_col=COL_TIMESTAMP):\n",
        "    train_mask = df[timestamp_col]<=split_date\n",
        "    val_mask = df[timestamp_col]>split_date\n",
        "    return train_mask, val_mask\n",
        "\n",
        "split_date = get_split_date_from_ratio(merged_df[merged_df[COL_SOURCE]==VAL_SOURCE_TRAIN], 0.8)\n",
        "print(\"Split date:\", split_date)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cOIxSFLlHaE",
        "outputId": "6b448397-e289-4f76-8c23-27cf123edecf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Данные загружены и объединены\n",
            "Split date: 2020-09-27 16:17:15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_bert_features(df, train_df, descriptions_df):\n",
        "    BERT_EMBEDDINGS_CACHE_PATH = Path(\"/content/bert_embeddings.pkl\")\n",
        "    embeddings_path = BERT_EMBEDDINGS_CACHE_PATH\n",
        "\n",
        "    if embeddings_path.exists():\n",
        "        print(\"Загрузка BERT эмбеддингов из:\", embeddings_path)\n",
        "        embeddings_dict = joblib.load(embeddings_path)\n",
        "    else:\n",
        "        print(\"Файл BERT эмбеддингов не найден. Генерим. Может быть долго на cpu\")\n",
        "\n",
        "        device = BERT_DEVICE\n",
        "        tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
        "        model = AutoModel.from_pretrained(BERT_MODEL_NAME)\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        all_descriptions = descriptions_df[[COL_BOOK_ID, COL_DESCRIPTION]].copy()\n",
        "        all_descriptions[COL_DESCRIPTION] = all_descriptions[COL_DESCRIPTION].fillna(\"\")\n",
        "        unique_books = all_descriptions.drop_duplicates(subset=[COL_BOOK_ID])\n",
        "\n",
        "        book_ids = unique_books[COL_BOOK_ID].to_numpy()\n",
        "        descriptions = unique_books[COL_DESCRIPTION].to_numpy().tolist()\n",
        "\n",
        "        embeddings_dict = {}\n",
        "        batch_size = BERT_BATCH_SIZE if device == \"cpu\" else BERT_BATCH_SIZE * 4\n",
        "        num_batches = (len(descriptions) + batch_size - 1) // batch_size\n",
        "\n",
        "        for batch_idx in tqdm(range(num_batches), desc=\"BERT batches\", unit=\"batch\"):\n",
        "            start = batch_idx * batch_size\n",
        "            end = min(start + batch_size, len(descriptions))\n",
        "            batch_desc = descriptions[start:end]\n",
        "            batch_ids = book_ids[start:end]\n",
        "\n",
        "            encoded = tokenizer(batch_desc,\n",
        "                                padding=True,\n",
        "                                truncation=True,\n",
        "                                max_length=BERT_MAX_LENGTH,\n",
        "                                return_tensors=\"pt\")\n",
        "            encoded = {k: v.to(device) for k, v in encoded.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**encoded)\n",
        "                mask = encoded[\"attention_mask\"].unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
        "                sum_emb = torch.sum(outputs.last_hidden_state * mask, dim=1)\n",
        "                sum_mask = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
        "                mean_emb = (sum_emb / sum_mask).cpu().numpy()\n",
        "\n",
        "            for book_id, emb in zip(batch_ids, mean_emb):\n",
        "                embeddings_dict[book_id] = emb\n",
        "\n",
        "        joblib.dump(embeddings_dict, embeddings_path)\n",
        "        print(\"BERT эмбеддинги сгенерированы и сохранены в:\", embeddings_path)\n",
        "\n",
        "    if not embeddings_dict:\n",
        "        print(\"Словарь эмбеддингов пуст. BERT фичи не будут добавлены.\")\n",
        "        return df\n",
        "\n",
        "    embeddings_list = []\n",
        "    for book_id in df[COL_BOOK_ID].to_numpy():\n",
        "        embeddings_list.append(embeddings_dict.get(book_id, np.zeros(BERT_EMBEDDING_DIM)))\n",
        "\n",
        "    bert_df = pd.DataFrame(np.array(embeddings_list),\n",
        "                           columns=[f\"bert_{i}\" for i in range(BERT_EMBEDDING_DIM)],\n",
        "                           index=df.index)\n",
        "\n",
        "    print(f\"Добавлено {BERT_EMBEDDING_DIM} BERT признаков.\")\n",
        "    return pd.concat([df.reset_index(drop=True), bert_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "featured_df = add_bert_features(merged_df, merged_df[merged_df[COL_SOURCE]==VAL_SOURCE_TRAIN], descriptions_df)\n",
        "print(\"BERT фичи добавлены\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARLIPQjXlPN9",
        "outputId": "e68721d1-7762-4776-cc06-783afb5b721d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загрузка BERT эмбеддингов из: /content/bert_embeddings.pkl\n",
            "Добавлено 768 BERT признаков.\n",
            "BERT фичи добавлены\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "processed_path = PROCESSED_DATA_DIR / PROCESSED_DATA_FILENAME\n",
        "featured_df.to_parquet(processed_path, index=False, engine=\"pyarrow\", compression=\"snappy\")\n",
        "print(\"Обработанные данные сохранены:\", processed_path)\n",
        "\n",
        "# === Функции для признаков ===\n",
        "def add_book_features(df, ref_df):\n",
        "    train_raw = pd.read_csv(RAW_DATA_DIR / TRAIN_FILENAME, usecols=[COL_BOOK_ID, COL_HAS_READ])\n",
        "\n",
        "    book_popularity = train_raw[train_raw[COL_HAS_READ] == 1].groupby(COL_BOOK_ID).size().reset_index(name='book_popularity_count')\n",
        "    book_interaction = train_raw.groupby(COL_BOOK_ID).size().reset_index(name='book_interaction_count')\n",
        "\n",
        "    book_features = book_popularity.merge(book_interaction, on=COL_BOOK_ID, how='outer').fillna(0)\n",
        "    book_features['book_read_ratio'] = book_features['book_popularity_count'] / book_features['book_interaction_count'].replace(0, np.nan)\n",
        "    book_features['book_read_ratio'] = book_features['book_read_ratio'].fillna(0)\n",
        "\n",
        "    df = df.merge(book_features, on=COL_BOOK_ID, how='left').fillna({\n",
        "        'book_popularity_count': 0,\n",
        "        'book_interaction_count': 0,\n",
        "        'book_read_ratio': 0\n",
        "    })\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_author_features(df, ref_df):\n",
        "    train_raw = pd.read_csv(RAW_DATA_DIR / TRAIN_FILENAME, usecols=[COL_BOOK_ID, COL_HAS_READ, COL_TARGET])\n",
        "    train_raw = train_raw.merge(ref_df[[COL_BOOK_ID, COL_AUTHOR_ID]].drop_duplicates(), on=COL_BOOK_ID, how='left')\n",
        "\n",
        "    author_avg = train_raw[(train_raw[COL_HAS_READ] == 1) & train_raw[COL_TARGET].notna()].groupby(COL_AUTHOR_ID)[COL_TARGET].mean().reset_index(name='author_avg_rating')\n",
        "    author_books = train_raw.groupby(COL_AUTHOR_ID)[COL_BOOK_ID].nunique().reset_index(name='author_books_count')\n",
        "    author_reads = train_raw[train_raw[COL_HAS_READ] == 1].groupby(COL_AUTHOR_ID).size().reset_index(name='author_reads_count')\n",
        "\n",
        "    author_features = author_avg.merge(author_books, on=COL_AUTHOR_ID, how='outer')\n",
        "    author_features = author_features.merge(author_reads, on=COL_AUTHOR_ID, how='outer').fillna(0)\n",
        "\n",
        "    author_features['author_popularity_score'] = (author_features['author_reads_count'] * author_features['author_avg_rating']) / author_features['author_books_count'].replace(0, np.nan)\n",
        "    author_features['author_popularity_score'] = author_features['author_popularity_score'].fillna(0)\n",
        "\n",
        "    df = df.merge(author_features, on=COL_AUTHOR_ID, how='left').fillna({\n",
        "        'author_avg_rating': 0,\n",
        "        'author_books_count': 0,\n",
        "        'author_reads_count': 0,\n",
        "        'author_popularity_score': 0\n",
        "    })\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_user_bias_features(df, train_df):\n",
        "    train_read = train_df[train_df[COL_HAS_READ] == 1].copy()\n",
        "\n",
        "    user_std = train_read.groupby(COL_USER_ID)[COL_RATING].std().reset_index()\n",
        "    user_std.columns = [COL_USER_ID, 'user_rating_std']\n",
        "    user_std['user_rating_std'] = user_std['user_rating_std'].fillna(0)\n",
        "\n",
        "    df = df.merge(user_std, on=COL_USER_ID, how='left')\n",
        "    df['user_rating_std'] = df['user_rating_std'].fillna(0)\n",
        "\n",
        "    global_mean_rating = train_read[COL_RATING].mean()\n",
        "\n",
        "    if F_USER_MEAN_RATING in df.columns:\n",
        "        df['user_global_bias'] = df[F_USER_MEAN_RATING].fillna(global_mean_rating) - global_mean_rating\n",
        "    else:\n",
        "        user_mean = train_read.groupby(COL_USER_ID)[COL_RATING].mean().reset_index()\n",
        "        user_mean.columns = [COL_USER_ID, 'temp_user_mean']\n",
        "        df = df.merge(user_mean, on=COL_USER_ID, how='left')\n",
        "        df['user_global_bias'] = df['temp_user_mean'].fillna(global_mean_rating) - global_mean_rating\n",
        "        df = df.drop(columns=['temp_user_mean'])\n",
        "\n",
        "    if F_USER_MEAN_RATING in df.columns and F_BOOK_MEAN_RATING in df.columns:\n",
        "        df['user_book_rating_diff'] = df[F_USER_MEAN_RATING].fillna(global_mean_rating) - df[F_BOOK_MEAN_RATING].fillna(global_mean_rating)\n",
        "    else:\n",
        "        df['user_book_rating_diff'] = 0\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_aggregate_features(df, ref_df):\n",
        "    user_agg = ref_df.groupby(COL_USER_ID)[COL_TARGET].agg(['mean', 'count']).reset_index()\n",
        "    user_agg.columns = [COL_USER_ID, F_USER_MEAN_RATING, F_USER_RATINGS_COUNT]\n",
        "    df = df.merge(user_agg, on=COL_USER_ID, how='left')\n",
        "\n",
        "    book_agg = ref_df.groupby(COL_BOOK_ID)[COL_TARGET].agg(['mean', 'count']).reset_index()\n",
        "    book_agg.columns = [COL_BOOK_ID, F_BOOK_MEAN_RATING, F_BOOK_RATINGS_COUNT]\n",
        "    df = df.merge(book_agg, on=COL_BOOK_ID, how='left')\n",
        "\n",
        "    author_agg = ref_df.groupby(COL_AUTHOR_ID)[COL_TARGET].agg('mean').reset_index()\n",
        "    author_agg.columns = [COL_AUTHOR_ID, F_AUTHOR_MEAN_RATING]\n",
        "    df = df.merge(author_agg, on=COL_AUTHOR_ID, how='left')\n",
        "\n",
        "    genres_count = book_genres_df.groupby(COL_BOOK_ID)[COL_GENRE_ID].nunique().reset_index()\n",
        "    genres_count.columns = [COL_BOOK_ID, F_BOOK_GENRES_COUNT]\n",
        "    df = df.merge(genres_count, on=COL_BOOK_ID, how='left')\n",
        "\n",
        "    return df\n",
        "\n",
        "def handle_missing_values(df, ref_df):\n",
        "    for col in CAT_FEATURES:\n",
        "        if col in df.columns:\n",
        "            if df[col].dtype.name == 'category':\n",
        "                df[col] = df[col].astype(object)\n",
        "            df[col] = df[col].fillna(MISSING_CAT_VALUE).astype(str)\n",
        "\n",
        "    num_agg_features = [F_USER_MEAN_RATING, F_BOOK_MEAN_RATING, F_AUTHOR_MEAN_RATING,\n",
        "                         F_USER_RATINGS_COUNT, F_BOOK_RATINGS_COUNT, F_BOOK_GENRES_COUNT,\n",
        "                         'book_popularity_count', 'book_interaction_count', 'book_read_ratio',\n",
        "                         'author_avg_rating', 'author_books_count', 'author_reads_count', 'author_popularity_score',\n",
        "                         'user_rating_std', 'user_global_bias', 'user_book_rating_diff']\n",
        "    for col in num_agg_features:\n",
        "        if col in df.columns:\n",
        "            mean_val = ref_df[col].mean(skipna=True) if col in ref_df.columns else MISSING_NUM_VALUE\n",
        "            df[col] = df[col].fillna(mean_val)\n",
        "\n",
        "    other_nums = df.select_dtypes(include=['float', 'int']).columns.difference(\n",
        "        num_agg_features + [COL_TARGET, COL_PREDICTION, COL_USER_ID, COL_BOOK_ID]\n",
        "    )\n",
        "    for col in other_nums:\n",
        "        df[col] = df[col].fillna(MISSING_NUM_VALUE)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTAJ24znleqH",
        "outputId": "c0ec598a-493a-4cd2-9010-6af5b97a882a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Обработанные данные сохранены: data/processed/processed_features.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "    df = pd.read_parquet(PROCESSED_DATA_DIR / PROCESSED_DATA_FILENAME)\n",
        "    train_set = df[df[COL_SOURCE] == VAL_SOURCE_TRAIN].copy()\n",
        "\n",
        "    split_date = get_split_date_from_ratio(train_set, 0.8)\n",
        "    train_mask, val_mask = temporal_split_by_date(train_set, split_date)\n",
        "\n",
        "    train_split = train_set[train_mask].copy()\n",
        "    val_split = train_set[val_mask].copy()\n",
        "\n",
        "    train_split = add_aggregate_features(train_split, train_split)\n",
        "    val_split = add_aggregate_features(val_split, train_split)\n",
        "\n",
        "    train_split = add_book_features(train_split, train_split)\n",
        "    train_split = add_author_features(train_split, train_split)\n",
        "    train_split = add_user_bias_features(train_split, train_split)\n",
        "\n",
        "    val_split = add_book_features(val_split, train_split)\n",
        "    val_split = add_author_features(val_split, train_split)\n",
        "    val_split = add_user_bias_features(val_split, train_split)\n",
        "\n",
        "    train_split = handle_missing_values(train_split, train_split)\n",
        "    val_split = handle_missing_values(val_split, train_split)\n",
        "\n",
        "    exclude_cols = [COL_SOURCE, COL_TARGET, COL_PREDICTION, COL_TIMESTAMP]\n",
        "    features = [c for c in train_split.columns if c not in exclude_cols]\n",
        "\n",
        "    non_feature_object_cols = train_split[features].select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "    features = [f for f in features if f not in non_feature_object_cols]\n",
        "\n",
        "    X_train = train_split[features]\n",
        "    y_train = train_split[COL_TARGET]\n",
        "    X_val = val_split[features]\n",
        "    y_val = val_split[COL_TARGET]\n",
        "\n",
        "    print(f\"Train shape: {X_train.shape}, Val shape: {X_val.shape}\")\n",
        "    print(f\"Features: {len(features)}\")\n",
        "\n",
        "    MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    model = lgb.LGBMRegressor(**LGB_PARAMS)\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)],\n",
        "        eval_metric=\"rmse\"\n",
        "    )\n",
        "\n",
        "    model.booster_.save_model(str(MODEL_DIR / MODEL_FILENAME))\n",
        "    joblib.dump(features, MODEL_DIR / \"feature_names.pkl\")\n",
        "\n",
        "    print(\"Модель обучена и сохранена\")\n",
        "\n",
        "    val_preds = model.predict(X_val)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
        "    mae = mean_absolute_error(y_val, val_preds)\n",
        "    score = 1 - 0.5 * (rmse / 10 + mae / 10)\n",
        "    print(f\"Validation RMSE: {rmse:.4f}, MAE: {mae:.4f}, Score: {score:.4f}\")\n",
        "\n",
        "train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-VXWFR-lo2e",
        "outputId": "a9faacce-3e87-4875-ff76-f7239f3f1276"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (124944, 786), Val shape: (31235, 786)\n",
            "Features: 786\n",
            "Модель обучена и сохранена\n",
            "Validation RMSE: 2.8987, MAE: 2.1122, Score: 0.7495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_save_submission():\n",
        "    df = pd.read_parquet(PROCESSED_DATA_DIR / PROCESSED_DATA_FILENAME)\n",
        "    train_set = df[df[COL_SOURCE] == VAL_SOURCE_TRAIN].copy()\n",
        "    test_set = df[df[COL_SOURCE] == VAL_SOURCE_TEST].copy()\n",
        "\n",
        "    train_set = add_aggregate_features(train_set, train_set)\n",
        "    test_set = add_aggregate_features(test_set, train_set)\n",
        "\n",
        "    train_set = add_book_features(train_set, train_set)\n",
        "    train_set = add_author_features(train_set, train_set)\n",
        "    train_set = add_user_bias_features(train_set, train_set)\n",
        "\n",
        "    test_set = add_book_features(test_set, train_set)\n",
        "    test_set = add_author_features(test_set, train_set)\n",
        "    test_set = add_user_bias_features(test_set, train_set)\n",
        "\n",
        "    train_set = handle_missing_values(train_set, train_set)\n",
        "    test_set = handle_missing_values(test_set, train_set)\n",
        "\n",
        "    features = joblib.load(MODEL_DIR / \"feature_names.pkl\")\n",
        "\n",
        "    missing_features = set(features) - set(test_set.columns)\n",
        "    if missing_features:\n",
        "        print(f\"Отсутствующие признаки: {missing_features}\")\n",
        "        for feat in missing_features:\n",
        "            test_set[feat] = 0\n",
        "\n",
        "    X_test = test_set[features]\n",
        "\n",
        "    model = lgb.Booster(model_file=str(MODEL_DIR / MODEL_FILENAME))\n",
        "\n",
        "    test_preds = model.predict(X_test)\n",
        "    clipped_preds = np.clip(test_preds, PREDICTION_MIN_VALUE, PREDICTION_MAX_VALUE)\n",
        "\n",
        "    submission_df = test_set[[COL_USER_ID, COL_BOOK_ID]].copy()\n",
        "    submission_df[COL_PREDICTION] = clipped_preds\n",
        "\n",
        "    submission_path = \"submission.csv\"\n",
        "    submission_df.to_csv(submission_path, index=False)\n",
        "    print(f\"Сабмит создан, победа\")\n",
        "\n",
        "predict_and_save_submission()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWDaLNkDlu_u",
        "outputId": "c6bfe568-6420-440a-fec3-80fdfb5daca6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сабмит создан, победа\n"
          ]
        }
      ]
    }
  ]
}